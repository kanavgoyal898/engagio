{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **engagio:** Engagement Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File Handling Cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from utility import *\n",
    "from architecture import SignalEncoder\n",
    "from imbalance import SelfAdjDiceLoss\n",
    "\n",
    "random.seed(2147483647)\n",
    "torch.manual_seed(2147483647)\n",
    "\n",
    "PATH = 'DAiSEE/DataSet/'\n",
    "\n",
    "FPS = 30\n",
    "BATCH_SIZE = 16\n",
    "VIDEO_LENGTH = 10\n",
    "FRAME_INTERVAL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visual Features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste in your code here @AlakhsimarSingh\n",
    "\n",
    "# fan_in_A = number of input features\n",
    "fan_in_A = 0    # replace 0 with the actual value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Physiological Features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste in your code here @NischayVerma\n",
    "\n",
    "# fan_in_N = number of input features\n",
    "fan_in_N = 0    # replace 0 with the actual value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Device Auto Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngagementModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, fan_in_A, fan_in_N, fan_out_A, fan_out_N, fan_out, n_embd, head_size, n_heads, n_blocks, ffwd_mul):\n",
    "        super().__init__()\n",
    "        self.fan_out = fan_out\n",
    "        self.signal_encoder_A = SignalEncoder(fan_in_A, fan_out_A, n_embd, head_size, n_heads, n_blocks, ffwd_mul)\n",
    "        self.signal_encoder_N = SignalEncoder(fan_in_N, fan_out_N, n_embd, head_size, n_heads, n_blocks, ffwd_mul)\n",
    "        self.lm_head = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(fan_out_A + fan_out_N, fan_out),\n",
    "                            torch.nn.GELU()\n",
    "                        )\n",
    "\n",
    "    def forward(self, x_A, x_N, idx=None, weighted=False):\n",
    "        x_A = self.signal_encoder_A(x_A)\n",
    "        x_N = self.signal_encoder_N(x_N)\n",
    "        \n",
    "        out = torch.cat([x_A, x_N], -1)\n",
    "        out = self.lm_head(out)\n",
    "        B, T, C = out.shape\n",
    "\n",
    "        loss = None\n",
    "        if idx is not None:\n",
    "            x = out.float()\n",
    "            y = idx.unsqueeze(1).repeat(1, T, 1).float()\n",
    "            x = x.view(-1, self.fan_out)\n",
    "            y = y.view(-1).long()\n",
    "            criterion = SelfAdjDiceLoss() if weighted else torch.nn.CrossEntropyLoss()\n",
    "            loss = criterion(x, y)\n",
    "            # loss = torch.nn.functional.cross_entropy(x, y, weight=torch.tensor([1/61, 1/455, 1/4422, 1/3987], dtype=x.dtype, device=x.device) if weighted else None)\n",
    "        \n",
    "        return out, loss\n",
    "\n",
    "# NOTE: n_embd = head_size * n_heads\n",
    "model = EngagementModel(fan_in_A, fan_in_N, 4, 4, 4, 64, 16, 4, 8, 4)\n",
    "model.to(device)\n",
    "\n",
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, true_labels):\n",
    "    logits = logits.to(device)\n",
    "    true_labels = true_labels.to(device)\n",
    "\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(-1, C)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)  \n",
    "    \n",
    "    predicted_classes = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "    predicted_classes = predicted_classes.view(B, T, 1)\n",
    "    true_labels = true_labels.unsqueeze(1).repeat(1, T, 1)\n",
    "    \n",
    "    correct_predictions = (predicted_classes == true_labels).float().sum()\n",
    "    total_predictions = B*T\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acci = []\n",
    "lossi = []\n",
    "\n",
    "lr_i = 1e-3\n",
    "lr_f = 1e-6\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    lr = 1e-3 + ((lr_f - lr_i)/iters)*i\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    X_batch, Y_batch = load_data(PATH, 'Train', BATCH_SIZE)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # code for X_batch_A, X_batch_N\n",
    "    X_batch_A = X_batch                 # replace X_batch_A with the actual code @AlakhsimarSingh\n",
    "    X_batch_N = X_batch                 # replace X_batch_N with the actual code @NischayVerma\n",
    "    X_batch_A.requires_grad = True    \n",
    "    X_batch_N.requires_grad = True    \n",
    "    \n",
    "    logits, loss = model(X_batch_A.to(device), X_batch_N.to(device), Y_batch.to(device), weighted=True)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    acc = compute_accuracy(logits, Y_batch)\n",
    "    print(f'{i} iteration, loss: {loss:.4f}, acc: {acc:.4f}')\n",
    "\n",
    "    acci.append(acc)\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Average loss: {sum(lossi)/len(lossi):.4f}, Average accuracy: {sum(acci)/len(acci):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.plot(lossi, color='blue')\n",
    "ax1.set_title('Loss Plot')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Lossi Values')\n",
    "\n",
    "ax2.plot(acci, color='green')\n",
    "ax2.set_title('Acci Plot')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Metrics on Validation Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acci = []\n",
    "lossi = []\n",
    "for i in range(10):\n",
    "    X_batch, Y_batch = load_data(PATH, 'Validation', BATCH_SIZE)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # code for X_batch_A, X_batch_N\n",
    "    X_batch_A = X_batch                 # replace X_batch_A with the actual code @AlakhsimarSingh\n",
    "    X_batch_N = X_batch                 # replace X_batch_N with the actual code @NischayVerma\n",
    "    X_batch_A.requires_grad = True    \n",
    "    X_batch_N.requires_grad = True    \n",
    "    \n",
    "    logits, loss = model(X_batch_A.to(device), X_batch_N.to(device), Y_batch.to(device), weighted=True)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    logits, loss = model(X_batch.to(device), X_batch.to(device), Y_batch.to(device))\n",
    "\n",
    "    acc = compute_accuracy(logits, Y_batch)\n",
    "\n",
    "    acci.append(acc)\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "print(f'Average loss: {sum(lossi)/len(lossi):.4f}, Average accuracy: {sum(acci)/len(acci):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Late Fusion Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @AlakhsimarSingh ML code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @NischayVerma ML code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateFusion(torch.nn.Module):\n",
    "    def __init__(self, num_modalities, num_classses, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classses\n",
    "        self.num_modalities = num_modalities\n",
    "        self.wei = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.num_modalities, 1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, logits, y=None):\n",
    "        out = self.wei(logits)\n",
    "        out = out.squeeze(-1)\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            x = out.float()\n",
    "            y = y.long()\n",
    "            loss = torch.nn.functional.cross_entropy(x, y)\n",
    "        return out, loss\n",
    "    \n",
    "model = LateFusion(2, 4, 64)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_accuracy(logits, true_labels):\n",
    "    logits = logits.to(device)\n",
    "    true_labels = true_labels.to(device)\n",
    "    \n",
    "    B, C = logits.shape\n",
    "    logits = logits.view(-1, C)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    predicted_classes = torch.argmax(probabilities, dim=-1)\n",
    "    \n",
    "    correct_predictions = (predicted_classes == true_labels).float().sum()\n",
    "    total_predictions = B\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acci = []\n",
    "lossi = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(1000):\n",
    "    X_batch, Y_batch = load_data(PATH, 'Train', BATCH_SIZE)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # add your signals here\n",
    "    X_batch_A = X_batch     # @AlakhsimarSingh signals\n",
    "    X_batch_N = X_batch     # @NischayVerma signals\n",
    "    # ------------------------------------------\n",
    "\n",
    "    X_batch = torch.stack([X_batch_A, X_batch_N], dim=-1)\n",
    "    logits, loss = model(X_batch.to(device), Y_batch.to(device))\n",
    "\n",
    "    acc = _compute_accuracy(logits, Y_batch)\n",
    "    print(f'{i} iteration, loss: {loss.item():.4f}, acc: {acc:.4f}')\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    acci.append(acc)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Average loss: {sum(lossi)/len(lossi):.4f}, Average accuracy: {sum(acci)/len(acci):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.plot(lossi, color='blue')\n",
    "ax1.set_title('Loss Plot')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Lossi Values')\n",
    "\n",
    "ax2.plot(acci, color='green')\n",
    "ax2.set_title('Acci Plot')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acci = []\n",
    "lossi = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(1000):\n",
    "    X_batch, Y_batch = load_data(PATH, 'Validation', BATCH_SIZE)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # add your signals here\n",
    "    X_batch_A = X_batch     # @AlakhsimarSingh signals\n",
    "    X_batch_N = X_batch     # @NischayVerma signals\n",
    "    # ------------------------------------------\n",
    "\n",
    "    X_batch = torch.stack([X_batch_A, X_batch_N], dim=-1)\n",
    "    logits, loss = model(X_batch.to(device), Y_batch.to(device))\n",
    "\n",
    "    acc = _compute_accuracy(logits, Y_batch)\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    acci.append(acc)\n",
    "\n",
    "print(f'Average loss: {sum(lossi)/len(lossi):.4f}, Average accuracy: {sum(acci)/len(acci):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
